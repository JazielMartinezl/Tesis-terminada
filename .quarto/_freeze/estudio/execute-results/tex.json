{
  "hash": "d735ab98f47afb26122e4644ecf32298",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlang: es\n---\n\n\n\n\n\n\n# Estudio de caso\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Descripción del problema\n\nLa modelación de las variables climaticas permite el entendimiento y la generación del pronóstico climático de una región para la planificación territorial, agricultura, ecología, análisis y la conservación de los recursos naturales.\n\nEn el presente estudio, se utilizó un conjunto de datos climáticos que fueron consultados del 01 de Enero del 2009 al 01 de Enero del 2012. Con la finalidad de comprobar la utilidad y eficacia de dos modelos adecuados de redes neuronales: LTSM y la red neuronal convolucional (CNN). Para el pronóstico de datos climáticos modelados como series de tiempo.\n\n### Origen de los datos\n\n::: {style=\"text-align: justify\"}\nSe emplea un conjunto de datos con información climática que fue grabada con el objetivo de ser analizada en la estación climática en el Instituto de Biogeoquimica Max Planck en Jena, Alemania. El mismo contiene todas las variables medidas con su correspondiente instrumento (tal como temperatura del aire, presión atmosférica, humedad, dirección del viento, etc) grabadas cada 10 minutos, a lo largo de varios años. El cual está disponible para su descarga en <https://www.bgc-jena.mpg.de/wetter/weather_data.html> .\n\nDe los 14 datos diferentes, se seleccionan 5 teniendo en cuenta factores como su rango y sus patrones estacionales.\n\nLas variables seleccionadas son las siguientes:\n\n-   $p( mbar )$ - Presión del aire.\n\n-   $T( ° C )$ - Temperatura del aire.\n\n-   $rh( \\% )$ - Humedad relativa.\n\n-   $rho(g/m^3$) - Densidad del aire\n\n-   $wv(m/s)$ - Velocidad del viento.\n\n<!-- Se presentan las 5 variables en su serie temporal. -->\n:::\n\n## Limpieza y procesamiento de datos\n\n::: {style=\"text-align: justify\"}\nLos datos recopilados fueron sometidos a un proceso exhaustivo de limpieza y preprocesamiento. <!-- Para conformar la base de datos que se utilizo,  --> En este paso se detectó el intervalo de tiempo en el que mejor se comportarán los datos históricos, por ello se asignó un conjunto de datos históricos omitiendo los primeros 7 días, así como la estandarización y normalización de las 5 variables mas relevantes. Esto con el objetivo de obtener un mejor pronóstico de los datos a futuro.\n:::\n\n## Pronóstico mediante red neuronales\n\n### Modelamiento y pronóstico mediante redes neuronales para series de tiempo unidimensionales\n\n::: {style=\"text-align: justify\"}\nSe presentan las gráficas de los datos históricos de las 5 variables elegidas.\n:::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-Temperatura_ts}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTemp<-df_clima$T..degC.\nTemp <-Temp +273.15\nDatos_historicos <- Temp \nX<- df_clima$Date.Time\nfig <- plot_ly() %>%\n  add_trace(x = ~X, y = ~Datos_historicos, type = \"scatter\", mode = \"lines\", name = \"Temperatura\", line = list(color = '#63B8FF')) %>%\n  layout(title = \" \",\n         xaxis = list(title = \" \"),\n         yaxis = list(title = \"Temperatura (°F)\")) \nfig\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSerie de tiempo de la Temperatura (° F) del 01 de Enero del 2009 al 01 de Enero del 2012.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-paire_ts}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_aire <-df_clima$p..mbar. #presion del aire\nX<- df_clima$Date.Time\nfig <- plot_ly() %>%\n  add_trace(x = ~X, y = ~p_aire, type = \"scatter\", mode = \"lines\", name = \"presión aire\", line = list(color = '#63B8FF')) %>%\n  layout(title = \" \",\n         xaxis = list(title = \" \"),\n         yaxis = list(title = \"mbar\")) \nfig\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSerie de tiempo de la presión del aire p (mbar) del 01 de Enero del 2009 al 01 de Enero del 2012.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-hrelativa_ts}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_relativa<-df_clima$rh.... #humedad relativa\nX<- df_clima$Date.Time\nfig <- plot_ly() %>%\n  add_trace(x = ~X, y = ~h_relativa, type = \"scatter\", mode = \"lines\", name = \"Humedad relativa\", line = list(color = '#63B8FF')) %>%\n  layout(title = \" \",\n         xaxis = list(title = \" \"),\n         yaxis = list(title = \"Porcentaje\")) \nfig\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSerie de tiempo de la humedad relativa rh (%) del 01 de Enero del 2009 al 01 de Enero del 2012.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-daire_ts}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_aire<-df_clima$rho..g.m..3. # Densidad del aire \nX<- df_clima$Date.Time\nfig <- plot_ly() %>%\n  add_trace(x = ~X, y = ~d_aire, type = \"scatter\", mode = \"lines\", name = \"Humedad relativa\", line = list(color = '#63B8FF')) %>%\n  layout(title = \" \",\n         xaxis = list(title = \" \"),\n         yaxis = list(title = \"g /m³\")) \nfig\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSerie de tiempo de la densidad del aire rho ($g /m^3$) del 01 de Enero del 2009 al 01 de Enero del 2012.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-viento_ts}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nv_viento <-df_clima$wv..m.s. # Velocidad del viento\nX<- df_clima$Date.Time\nfig <- plot_ly() %>%\n  add_trace(x = ~X, y = ~v_viento, type = \"scatter\", mode = \"lines\", name = \"Humedad relativa\", line = list(color = '#63B8FF')) %>%\n  layout(title = \" \",\n         xaxis = list(title = \" \"),\n         yaxis = list(title = \"g/m\")) \nfig\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nSerie de tiempo de la velocidad del viento wv ($m/s$) del 01 de Enero del 2009 al 01 de Enero del 2012.\n:::\n::::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Serie de tiempo de la Temperatura(° F) del 01 de Enero del 2009 al 01 de Enero del 2012.](estudio_files/figure-pdf/fig-Temperatura_tspdf-1.pdf){#fig-Temperatura_tspdf}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Serie de tiempo de la presión del aire p(mbar) del 01 de Enero del 2009 al 01 de Enero del 2012.](estudio_files/figure-pdf/fig-paire_tspdf-1.pdf){#fig-paire_tspdf}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Serie de tiempo de la humedad relativa rh (%) del 01 de Enero del 2009 al 01 de Enero del 2012.](estudio_files/figure-pdf/fig-hrelativa_tspdf-1.pdf){#fig-hrelativa_tspdf}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Serie de tiempo de la densidad del aire rho($g /m^3$) del 01 de Enero del 2009 al 01 de Enero del 2012.](estudio_files/figure-pdf/fig-daire_tspdf-1.pdf){#fig-daire_tspdf}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Serie de tiempo de la velocidad del viento wv($m/ s$) del 01 de Enero del 2009 al 01 de Enero del 2012.](estudio_files/figure-pdf/fig-viento_tspdf-1.pdf){#fig-viento_tspdf}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n\\newpage\n:::\n\n#### Estacionalidad\n\n::: {style=\"text-align: justify\"}\nCon el propósito de llevar a cabo una inspección visual y analítica de los datos y al mismo tiempo una descripción preliminar, se busca identificar los patrones de estacionalidad anual y diaria. Esto se puede apreciar claramente en los gráficos de [autocorrelación](series.qmd#sec-autocorrelación). Para calcular la autocorrelación en R se utiliza la función acf(). Esta función proporciona estimaciones gráficamente de la autocorrelación que se puede representar el conjunto de datos históricos de la Temperatura (° F).\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\nEl código que genera la autocorrelación es el siguiente :\n:::\n\n::: {#fig-Temp200}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(df_clima$T..degC., lag.max = 200, \n    main = \"Autocorrelación de Temperatura\")\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAutocorrelación de los primeros 200 datos correspondiente a la Temperatura (° F) en su serie temporal.\n:::\n\n::: {style=\"text-align: justify\"}\nEn @fig-Temp200 se observa que la autocorrelación los primeros 200 datos correspondiente a la Temperatura (° F) vistas como serie temporal. Teniendo en cuenta que el intervalo temporal entre observaciones es de 10 minutos, el patrón cíclico que se observa cada 144 observaciones corresponde al transcurso de 24 horas.\n:::\n\n::: {#fig-Temp1000}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(df_clima$T..degC., lag.max = 1000, \n    main = \"Autocorrelación de Temperatura\")\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAutocorrelación de los primeros 1000 datos correspondiente a la Temperatura (° F) en su serie temporal.\n:::\n\n::: {style=\"text-align: justify\"}\nEn @fig-Temp200 se observa que la autocorrelación los primeros 1000 datos correspondiente a la Temperatura (° F) en su serie temporal.\n:::\n\n::: {#fig-Temp100000}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacf(df_clima$T..degC., lag.max = 100000, \n    main = \"Autocorrelación de Temperatura\")\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nAutocorrelación de los primeros 100000 datos correspondiente a la Temperatura (° F) en su serie temporal.\n:::\n\n::: {style=\"text-align: justify\"}\nEn la @fig-Temp100000 podemos deducir que los datos históricos siguen un patrón cíclico que se observa es aproximadamente cada 52000 observaciones, es decir, corresponde al transcurso de un año.\n:::\n\n#### Prueba KPSS (Kwiatkowski-Phillips-Schmidt-Shin)\n\n::: {style=\"text-align: justify\"}\nA continuación, se emplea la prueba de Kwiatkowski-Phillips-Schmidt-Shin ([KPSS](series.qmd#sec-KPSS)) para examinar la presencia de estacionariedad en la serie temporal. Este test fué utilizado con la finalidad de identificar la existencia de [raices unitarias](series.qmd#sec-raices-unitarias) en la serie, lo cual permite inferir la presencia o ausencia de [estacionariedad](series.qmd#sec-series-estacionales) en los datos analizados.\n:::\n\nLa hipótesis nula y la alternativa para la prueba KPSS son:\n\n$$H_0:  El \\ modelo \\ es \\ estacionario\n\\ \\ \\qquad H_1: El \\  modelo \\  no \\ es \\  estacionario$$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(tseries)\nTemperatura_ts <-datos_clima$`T (degC)`[1:105120] #Temperatura\nkpss.test(Temperatura_ts, null = \"Trend\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKPSS Test for Trend Stationarity\n\ndata:  Temperatura_ts\nKPSS Trend = 21.053, Truncation lag parameter = 22, p-value = 0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nLa hipótesis nula $H_0$ no asume la presencia de raíces unitarias, lo que indica no estacionariedad en la serie, al obtener un KPSS estadístico superior que el nivel de significancia establecido el cual es de $5 \\%$ ($0.05$), se rechaza la hipótesis nula, sugiriendo la ausencia de estacionariedad en la serie de tiempo de Temperatura (° F). Por lo que vamos a utilizar técnicas para obtener la estacionariedad de la serie de tiempo.\n\nLuego, para hacer estacionarios los datos se aplicaron las siguientes transformaciones:\n:::\n\n#### Diferenciación\n\n::: {style=\"text-align: justify\"}\nSe aplicó las diferencias a los datos de Temperatura (° F) para buscar la estacionalidad de la serie de Tiempo.\n:::\n\n::: {#fig-Temp_diferencias}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(tswge)\nd_Temperatura = artrans.wge(Temperatura_ts,phi.tr= 1)\n```\n\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nDiferencias de los datos de Temperatura (° F) en su serie temporal.\n:::\n\n::: {style=\"text-align: justify\"}\nEn la @fig-Temp_diferencias se observa que después de aplicar diferencias a los datos de Temperatura (° F) observamos que la serie presenta estacionalidad.\n:::\n\n<!-- y de la prueba Kpss en la cual no se rechaza (la serie es estacionaria). -->\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(tseries)\n#d2_Temperatura = artrans.wge(d_Temperatura,phi.tr= 1)\nkpss.test(d_Temperatura, null = \"Trend\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKPSS Test for Trend Stationarity\n\ndata:  d_Temperatura\nKPSS Trend = 0.0026708, Truncation lag parameter = 22, p-value = 0.1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nAl aplicar la prueba KPSS a las diferencias a los datos de Temperatura (°F) en su serie de tiempo, se obtiene un kpss estadístico menor devuelto por el test, no se rechaza la hipótesis nula, confirmando la estacionariedad en la serie de tiempo de la Temperatura (°F).\n:::\n\n#### Estandarización\n\n::: {style=\"text-align: justify\"}\nPara que los datos sean comparables, es necesario estandarizarlos. Se ajustan los datos históricos en su serie de tiempo para que su media sea 0 y su desviación estándar sea 1.\n:::\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```{style=\"max-height: 300px;\"}\n  [1] -1.555912e+00 -3.988778e-01  7.980539e-01  1.596903e-01  8.778494e-01\n  [6]  1.715702e+00  9.944162e-05 -1.156935e+00 -2.074582e+00 -1.316525e+00\n [11] -4.786733e-01  1.197926e-01  7.989489e-02  6.783608e-01  9.944162e-05\n [16] -1.594915e-01 -4.387755e-01 -1.195937e-01 -3.988778e-01  9.944162e-05\n [21]  3.192812e-01 -5.185710e-01 -2.392869e-01 -7.180596e-01 -1.037241e+00\n [26] -3.979828e-02  5.985653e-01 -4.786733e-01 -6.382642e-01 -1.594915e-01\n [31] -3.979828e-02 -8.776505e-01 -3.979828e-02  1.596008e+00 -6.382642e-01\n [36]  3.192812e-01  1.755599e+00  9.576448e-01 -5.185710e-01 -1.555912e+00\n [41]  1.037440e+00  1.835395e+00  6.384630e-01  5.985653e-01  3.990767e-01\n [46]  6.783608e-01  1.197926e-01 -1.594915e-01  3.192812e-01  4.389744e-01\n [51]  8.379517e-01  6.783608e-01 -3.589801e-01 -1.993892e-01  5.985653e-01\n [56]  1.077338e+00  5.586676e-01  5.187699e-01 -7.969601e-02  3.999717e-02\n [61] -4.786733e-01 -4.387755e-01 -3.190824e-01 -3.979828e-02 -2.392869e-01\n [66] -2.791846e-01  3.990767e-01  6.783608e-01  1.995881e-01  5.187699e-01\n [71]  6.384630e-01  3.990767e-01  2.793835e-01  3.591790e-01  3.990767e-01\n [76]  1.197031e+00  1.276827e+00  2.394858e-01  2.793835e-01 -5.584687e-01\n [81] -2.791846e-01  3.591790e-01 -2.392869e-01  3.990767e-01  3.192812e-01\n [86]  3.999717e-02 -3.979828e-02  1.197926e-01  1.596903e-01  6.384630e-01\n [91] -1.594915e-01  5.586676e-01  4.389744e-01 -1.594915e-01 -1.594915e-01\n [96]  3.591790e-01  1.197926e-01 -1.594915e-01  1.197926e-01 -1.594915e-01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Normalización\n\n::: {style=\"text-align: justify\"}\nEsta transformación modifica la escala de los datos a un nuevo rango entre 0 y 1.\n:::\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```{style=\"max-height: 300px;\"}\n  [1] 0.1326531 0.4285714 0.7346939 0.5714286 0.7551020 0.9693878 0.5306122\n  [8] 0.2346939 0.0000000 0.1938776 0.4081633 0.5612245 0.5510204 0.7040816\n [15] 0.5306122 0.4897959 0.4183673 0.5000000 0.4285714 0.5306122 0.6122449\n [22] 0.3979592 0.4693878 0.3469388 0.2653061 0.5204082 0.6836735 0.4081633\n [29] 0.3673469 0.4897959 0.5204082 0.3061224 0.5204082 0.9387755 0.3673469\n [36] 0.6122449 0.9795918 0.7755102 0.3979592 0.1326531 0.7959184 1.0000000\n [43] 0.6938776 0.6836735 0.6326531 0.7040816 0.5612245 0.4897959 0.6122449\n [50] 0.6428571 0.7448980 0.7040816 0.4387755 0.4795918 0.6836735 0.8061224\n [57] 0.6734694 0.6632653 0.5102041 0.5408163 0.4081633 0.4183673 0.4489796\n [64] 0.5204082 0.4693878 0.4591837 0.6326531 0.7040816 0.5816327 0.6632653\n [71] 0.6938776 0.6326531 0.6020408 0.6224490 0.6326531 0.8367347 0.8571429\n [78] 0.5918367 0.6020408 0.3877551 0.4591837 0.6224490 0.4693878 0.6326531\n [85] 0.6122449 0.5408163 0.5204082 0.5612245 0.5714286 0.6938776 0.4897959\n [92] 0.6734694 0.6428571 0.4897959 0.4897959 0.6224490 0.5612245 0.4897959\n [99] 0.5612245 0.4897959\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: {style=\"text-align: justify\"}\nLuego del ajuste preliminar de los datos históricos en su serie de tiempo, se realizó una separación de los datos en dos grupos. Uno para entrenamiento y otro para testeo.\n:::\n\n## Implementación de la red neuronal LSTM {#sec-implementación-de-la-red-neuronal-lstm}\n\n::: {style=\"text-align: justify\"}\nEn esta parte, se realizó el siguiente ajuste para su implementación en lenguaje Python dentro del alterno [Colaboratory](Estadistica.qmd#sec-colab) de los servicios gratuitos de google.\n:::\n\n### Pronóstico mediante técnica rodante\n\nDe las los 144 datos por día. En cada paso, se actualiza la secuencia de entrada eliminando el valor más antiguo y agregando el último pronóstico como el valor más reciente. Esto se ilustra esquemáticamente en la @fig-rodante, donde $n$ es la longitud rodante de la secuencia de entrada y $T$ es la longitud de la serie Temporal.\n\n::: {#fig-rodante}\n$$\n\\begin{split}\ny:\\text{Observado}\\quad &\\quad \\hat{y}:\\text{Pronosticado}\\\\\ny_{T-n+1}\\quad y_{T-n+2}\\quad y_{T-n+3}\\quad&\\cdots\\quad y_{T-2}\\quad y_{T-1}\\quad y_T\\quad \\to\\quad \\color{orange}{\\hat{y}_{T+1}}\\\\\ny_{T-n+2}\\quad y_{T-n+3}\\quad y_{T-n+4}\\quad&\\cdots\\quad y_{T-1}\\quad y_{T}\\quad \\color{orange}{\\hat{y}_{T+1}}\\quad \\to\\quad \\color{orange}{\\hat{y}_{T+2}}\\\\\ny_{T-n+3}\\quad y_{T-n+4}\\quad y_{T-n+5}\\quad&\\cdots\\quad y_{T}\\quad \\color{orange}{\\hat{y}_{T+1}}\\quad \\color{orange}{\\hat{y}_{T+2}}\\quad \\to\\quad \\color{orange}{\\hat{y}_{T+3}}\\\\\ny_{T-n+4}\\quad y_{T-n+5}\\quad y_{T-n+6} \\quad & \\cdots\\quad\\color{orange}{\\hat{y}_{T+1}}\\quad\\color{orange}{\\hat{y}_{T+2}}\\quad\\color{orange}{\\hat{y}_{T+3}}  \\to\\quad \\color{orange}{\\hat{y}_{T+4}}\\\\\n&\\ddots \\\\\n\\end{split}\n$$\n:::\n\n### Entrenamiento y calibración del modelo LSTM\n\n::: {style=\"text-align: justify\"}\nSe procede al entrenamiento del modelo LSTM. La cantidad de capas ocultas son 3. La primera capa LSTM, es una capas LSTM con 50 unidades (neuronas). La segunda capa LSTM posee las mismas propiedades que la capa anterior. Y la tercera capa, es una una capa densa de salida. El tamaño del lote (batch size) es de 157, indica el número de muestras que se usarán para actualizar los pesos del modelo en cada paso de entrenamiento. Esta configuración se llevó a cabo con una función de activación [Adam](redes.qmd#sec-Adam), ejecutando 100 iteraciones para el entrenamiento de la red neuronal. La función de pérdida utilizada es el error cuadrático medio ([MSE](series.qmd#sec-MSE)).\n\nA continuación se exhibe el código utilizado.\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# univariate cnn example\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\npd.options.mode.chained_assignment = None\ntf.random.set_seed(0)\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Leer set de datos\nruta = '/content/drive/My Drive/Colab Notebooks/'\ndf = pd.read_csv(ruta+'datos_clima_wsu.csv')\nTimes = df['Date Time'] # Date\nTemperatura = df['T(C°)'] #Valiable\n#Tamaño de los Datos históricos\nTemperatura = Temperatura[(7*144):(22*144)] \nTemperatura = Temperatura + 273.15 \ny = pd.DataFrame({'Times': Times, 'Temperatura': Temperatura})\n# Convertir la columna 'fecha' al formato datetime de pandas\ny['Times'] = pd.to_datetime(y['Times'], format='%d.%m.%Y %H:%M:%S')\n\n# Establecer la columna 'fecha' como el índice de la serie de tiempo\ny.set_index('Times', inplace=True)\ny = y['Temperatura'].fillna(method='ffill')\ny = y.values.reshape(-1, 1)\n# Escalamos los datos\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(y)\ny = scaler.transform(y)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n``` python\n# CNN para datos univariados \nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\npd.options.mode.chained_assignment = None\ntf.random.set_seed(0)\n\nimport matplotlib.pyplot as plt\n\n# Grafica el Loss\ndef train_and_plot_loss(model, X, y, epochs=100, verbose=2):\n    history = model.fit(X, Y, epochs=100, \n    batch_size=314, verbose=2, shuffle=False)\n\n\n    # Graficar el loss\n    plt.plot(history.history['loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Leer set de datos\nruta = '/content/drive/My Drive/Colab Notebooks/'\ndf = pd.read_csv(ruta+'datos_clima_wsu.csv')\nTimes = df['Date Time'] # Date\nTemperatura = df['T(C°)'] #Valiable\n#Tamaño de los Datos historicos\nTemperatura = Temperatura[(7*144):(22*144)] \nTemperatura = Temperatura + 273.15 \ny = pd.DataFrame({'Times': Times, 'Temperatura': Temperatura})\n# Convertir la columna 'fecha' al formato datetime de pandas\ny['Times'] = pd.to_datetime(y['Times'], format='%d.%m.%Y %H:%M:%S')\n\n# Establecer la columna 'fecha' como el índice de la serie de tiempo\ny.set_index('Times', inplace=True)\ny = y['Temperatura'].fillna(method='ffill')\ny = y.values.reshape(-1, 1)\n# Escalamiento de  los datos\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(y)\ny = scaler.transform(y)\n\n# generar las secuencias de entrada y salida\nn_lookback = 6*144  #Secuancia de entrada\n# Secuancia de salida (o Número de predicciones)\nn_forecast = 144*4 \n\n# Inicializar DataFrame para almacenar los resultados\nresultados_df = pd.DataFrame()\nk = 50\n# Genera (k) simulaciones de la predicción\nfor repeticion in range(k):\n    X = []\n    Y = []\n\n    for i in range(n_lookback, len(y) - n_forecast + 1):\n        X.append(y[i - n_lookback: i])\n        Y.append(y[i: i + n_forecast])\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    # Crea el modelo\n    model = Sequential()\n    model.add(LSTM(units=50, return_sequences=True,\n              input_shape=(n_lookback, 1)))\n    model.add(LSTM(units=50))\n    model.add(Dense(n_forecast))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    #Entrenamiento del modelo\n    train_and_plot_loss(model, X, y, epochs=100, verbose=2)\n    \n\n    # Genera los pronósticos\n    X_ = y[-n_lookback:]\n    X_ = X_.reshape(1, n_lookback, 1)\n\n    Y_ = model.predict(X_).reshape(-1, 1)\n    Y_ = scaler.inverse_transform(Y_)\n\n    # Agregar resultados al DataFrame\n    resultados_df[f'Repeticion_{repeticion + 1}'] = Y_.flatten()\n\n# Mostrar el DataFrame con los resultados\nprint(resultados_df)\n```\n:::\n\n![Gráfica de la función de pérdida para la red neuronal LSTM](LOSS_LSTM.png){#fig-loss_LSTM}\n\n::: {style=\"text-align: justify\"}\nEn la @fig-loss_LSTM se observa la trayectoria del valor de la [función de pérdida](redes.qmd#sec-función-de-perdida) en cada época del entrenamiento en la red neuronal LSTM.\n:::\n\n## Implementación de la red neuronal convolucional (CNN) {#sec-implementación-de-la-red-neuronal-cnn.}\n\n::: {style=\"text-align: justify\"}\nEn esta parte, se realizó el mismo ajuste que en la red neuronal LSTM para su implementación en Python dentro de un servidor gratuito de [Colaboratory](Estadistica.qmd#sec-colab).\n\nSe procede al entrenamiento de la red neuronal convolucional (CNN). <!-- En esta parte se dividió en varios casos.  --> La cantidad de capas ocultas son 5 capas, de las cuales, la primera capa es una capa es convolucional unidimensional, donde va aprender a través de 64 mapas de características y con un tamaño de [kernel](redes.qmd#sec-kernel) de dimensión 5. La segunda capa es una Capa [Max-pooling](redes.qmd#sec-pooling) unidimensional, la tercera capa es una capa de aplanar ([flatten](redes.qmd#sec-fratten)), la cuarta capa es una capa Densa, la quinta capa es Densa (salida). Asimismo, se eligió la función de activación [ReLu](redes.qmd#sec-Relu), y el proceso de entrenamiento del modelo se ejecutó a lo largo de 25 iteraciones, 70 iteraciones y 100 iteraciones según los datos históricos.\n\nA continuación se exhibe el código utilizado.\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# univariate cnn example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\nfrom tensorflow.keras.optimizers import RMSprop\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler \n\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n\ndef train_and_plot_loss(model, X, y, epochs=20, verbose=1):\n    history = model.fit(X, y, epochs=epochs, verbose=verbose)\n    # Graficar el loss\n    plt.plot(history.history['loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    \n    \nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n# Leer set de datos\nruta = '/content/drive/My Drive/Colab Notebooks/'\ndf = pd.read_csv(ruta+'datos_clima_wsu.csv')\n\nTemperatura = df['T(C°)'] #Valiable\n#Tamaño de los Datos historicos\nTemperatura = Temperatura[(7*144):(37*144)] \nTemperatura = Temperatura + 273.15 \n\n# Escalar los datos entre 0 y 1\nscaler = MinMaxScaler(feature_range=(0, 1))\nDatos_historicos = scaler.fit_transform(Temperatura.values.reshape(-1, 1))\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n``` python\n# univariate cnn example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\nfrom tensorflow.keras.optimizers import RMSprop\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler    \n\n\n#Divide una secuencia univariada en muestras\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # Encuentra el final de este patrón\n        end_ix = i + n_steps\n        #Comprueba si se está más allá de la secuencia\n        if end_ix > len(sequence)-1:\n            break\n        #Reune las partes de entrada y salida del patrón\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\ndef train_and_plot_loss(model, X, y, epochs=20, verbose=1):\n    history = model.fit(X, y, epochs=epochs, verbose=verbose)\n    # Graficar el loss\n    plt.plot(history.history['loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    \n#Ubica el Drive del archivo    \nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n# Leer set de datos\nruta = '/content/drive/My Drive/Colab Notebooks/'\ndf = pd.read_csv(ruta+'datos_clima_wsu.csv')\nTemperatura = df['T(C°)'] #Valiable\n#Tamaño de los Datos históricos\nTemperatura = Temperatura[(7*144):(37*144)] \nTemperatura = Temperatura + 273.15 \n\n# Se escalan los datos entre 0 y 1\nscaler = MinMaxScaler(feature_range=(0, 1))\nDatos_historicos = scaler.fit_transform(Temperatura.values.reshape(-1, 1))\n```\n\n``` python\n#Define la secuencia de entrada\nraw_seq = Datos_historicos  \n# Cantidad de pasos de tiempo\nn_steps = 144\n# se divide las muestras\nX, y = split_sequence(raw_seq, n_steps)\nn_features = 1\n#[muestras, pasos de tiempo, características]\nX = X.reshape((X.shape[0],X.shape[1], n_features))  \n# Crear un modelo secuencial\nmodel = Sequential()\nmodel.add(Conv1D(64, kernel_size=5, \n  activation='relu',input_shape=(n_steps, n_features)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=RMSprop(clipvalue=1.0), loss='mae')\n# Entreniento\ntrain_and_plot_loss(model, X, y, epochs=100, verbose=1)\n```\n:::\n\n![Gráfica de la función de pérdida obtenida mediante una CNN con el conjunto de datos históricos de la variable Temperatura (°F).](loss_convolucional.png){#fig-loss_cnn}\n\n::: {style=\"text-align: justify\"}\nEn la @fig-loss_cnn se muestra la variación del valor de la [función de pérdida](redes.qmd#sec-función-de-perdida) a lo largo del entrenamiento de la red neuronal convolucional (CNN).\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n``` python\nnum_simulaciones = 50\nsimulaciones_df = pd.DataFrame()\n\nprediccion = []\n# Preparación de datos de entrada para la predicción\nx_input = y[(144*28):(144*29)]\nn = len(x_input)\nn_steps = 144\nn_features = 1\nx_input =  x_input.reshape((1, n_steps, n_features))\n\n# Predicción con el modelo entrenado\nyhat = model.predict(x_input, verbose=0).reshape(-1,1)\naux= yhat\naux = scaler.inverse_transform(aux)\n\n\nfor sim in range(num_simulaciones):\n    k = 144*4\n    prediccion = []\n    i = 0\n    for _ in range(k):\n        x_new = yhat[-n_steps:]\n        x_input = np.append(x_input, x_new)\n        x_input = x_input[-n_steps:]\n        x_input = x_input.reshape((1, len(x_input), n_features))\n        yhat = model.predict(x_input, verbose=0).reshape(-1,1)\n        aux= yhat\n        aux = scaler.inverse_transform(aux)\n        prediccion.append(aux)\n        print(\"Pronóstico para los próximos\", i, \"valores:\", aux)\n        i+= 1\n    # Agregar los resultados de la simulación actual al DataFrame\n    simulaciones_df[f'Simulacion_{sim+1}'] = prediccion\n    del model\n    raw_seq = Datos_historicos\n    n_steps = 144\n    X, y = split_sequence(raw_seq, n_steps)\n    n_features = 1\n    X = X.reshape((X.shape[0],X.shape[1], n_features))\n\n    model = Sequential()\n    model.add(Conv1D(64, kernel_size=5, activation='relu', \n    input_shape=(n_steps, n_features)))\n    model.add(MaxPooling1D(pool_size=5))\n    model.add(Flatten())\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer=RMSprop(clipvalue=1.0), loss='mae')\n    # Entrenamiento del modelo\n    model.fit(X, y, epochs=100, verbose=1)\n    x_input = y[(144*28):(144*29)]\n    n = len(x_input)\n    n_steps = 144\n    n_features = 1\n    x_input =  x_input.reshape((1, n_steps, n_features))\n    # Predicción con el modelo entrenado\n    yhat = model.predict(x_input, verbose=0).reshape(-1, 1)\n\n# Mostrar el DataFrame con los resultados de todas las simulaciones\nprint(simulaciones_df)\n```\n:::\n\n## Comparación de resultados\n\n::: {style=\"text-align: justify\"}\nDe manera similar, la implementación de los modelos LSTM y CNN sigue la misma metodología para las cuatro variables restantes, abarcando desde la estandarización de los datos hasta el pronóstico de los días seleccionados.\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n\\newpage\n:::\n\n## Resultados Finales del Estudio de Caso\n\n::: {style=\"text-align: justify\"}\nLos pronósticos obtenidos de la red neuronal LSTM y la red neuronal CNN, fueron guardados en hojas de cálculo para facilitar su posterior análisis. La visualización de la similitud y la tendencia de los pronósticos con los datos históricos, se hará mediante gráficas y tablas. Las gráficas fueron realizados con Plotly, versión para R, una librería para la visualización de datos en gráficos interactivos.\n:::\n\n::: {style=\"text-align: justify\"}\nPara analizar el pronóstico, se utilizan datos históricos correspondientes a los periodos de 180 días (6 meses), 90 días (3 meses), 30 días (un mes) y 15 días. Es importante destacar que, para cada entrenamiento, se calibraron los parámetros del modelo LSTM y del modelo CNN de manera específica, teniendo en cuenta la longitud de los datos históricos utilizados.\n:::\n\n::: {style=\"text-align: justify\"}\nEl estudio del comportamiento de los próximos 7 días posteriores a los 180 días (6 meses) de datos históricos, requirió la configuración de la [red neuronal convolucional](estudio.qmd#sec-implementación-de-la-red-neuronal-cnn), con 5 capas ocultas, se eligió la función de activación [ReLu](redes.qmd#sec-Relu) y el entrenamiento se ejecuta a 25 épocas para obtener 100 simulaciones. Por otro lado, con los mismos datos anteriores, se reconfiguró la [red neuronal LSTM](#sec-implementación-de-la-red-neuronal-lstm), con 3 capas ocultas tipo LSTM, se eligió el optimizador [Adam](redes.qmd#sec-Adam) con un tamaño de lote (batch size) de 314 y el entrenamiento se ejecuto a 25 épocas para obtener solo 10 simulaciones debido al largo tiempo de ejecución y que no se dispone de grandes recursos computacionales.\n\nPara el pronóstico de 4 días, se utilizaron 90 días (3 meses) de datos históricos, que en el caso de la configuración de la [red neuronal convolucional](estudio.qmd#sec-implementación-de-la-red-neuronal-cnn), tiene 5 capas ocultas, se eligió la función de activación [ReLu](redes.qmd#sec-Relu) y fueron ejecutadas 70 épocas para obtener 50 simulaciones. En cambio, para el caso de la [red neuronal LSTM](estudio.qmd#sec-implementación-de-la-red-neuronal-lstm) se utilizaron 3 capas ocultas tipo LSTM se eligió el optimizador [Adam](redes.qmd#sec-Adam) con un tamaño de lote (batch size) de 314 y el entrenamiento se ejecutó a 70 épocas para obtener las mismas 50 simulaciones.\n\nDe manera similar, para ver el comportamiento de los próximos 4 días posteriores a los 30 días de datos históricos, se configuró la [red neuronal convolucional](estudio.qmd#sec-implementación-de-la-red-neuronal-cnn) con 5 capas ocultas, se eligió la función de activación [ReLu](redes.qmd#sec-Relu) y el entrenamiento se ejecutó 100 épocas con el fin de obtener 100 simulaciones. Por otro lado, la configuración de la [red neuronal LSTM](estudio.qmd#sec-implementación-de-la-red-neuronal-lstm) utiliza 3 capas ocultas tipo LSTM, se eligió el optimizador [Adam](redes.qmd#sec-Adam) con un tamaño de lote (batch size) de 157 y el entrenamiento se realizó durante 100 épocas para generar las mismas 100 simulaciones.\n\nPara pronosticar los próximos 2 días, se tomaron 15 días de datos históricos. La configuración de la [red neuronal convolucional](estudio.qmd#sec-implementación-de-la-red-neuronal-cnn) tiene 5 capas ocultas, se eligió la función de activación [ReLu](redes.qmd#sec-Relu) y su entrenamiento se ejecutó por 40 épocas para obtener como en el caso anterior 100 simulaciones. Por otro lado, la [red neuronal LSTM](estudio.qmd#sec-implementación-de-la-red-neuronal-lstm) tiene 3 capas ocultas tipo LSTM, se eligió el optimizador [Adam](redes.qmd#sec-Adam) con un tamaño de lote (batch size) de 157 y el entrenamiento se ejecutó a 100 épocas con el fin de obtener las mismas 100 simulaciones.\n\nDado que el comportamiento de cada variable en su serie de tiempo es diferente, fué necesario hacer ajustes específicos en los parámetros para que la arquitectura de los modelos implementados fuera adecuada para realizar el pronóstico. Se tomó un número apropiado de épocas para que el valor en la función de pérdida fuera lo suficientemente pequeño, evitando así el [sobreentrenamiento del modelo](redes.qmd#sec-sobreentrenamiento) y asegurando la precisión en los pronósticos. Manteniendo al mismo tiempo las propiedades cíclicas de los datos históricos.\n:::\n\n::: {.content-visible when-format=\"html\" style=\"text-align: justify\"}\nTodo lo anterior se resume en las siguientes gráficas @fig-Temperatura , @fig-pmbar , @fig-rh, @fig-rho, @fig-wv.\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\nTodo lo anterior se resume en las siguientes gráficas @fig-Temperaturapdf , @fig-pmbarpdf , @fig-rhpdf, @fig-rhopdf, @fig-wvpdf.\n:::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-Temperatura}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-24-1.pdf)\n:::\n:::\n\n\n\n\n\n\nPredicción de 7 días, 4 días y 2 días de la Temperatura (° F) utilizando la red neuronal convolucional y la red neuronal LSTM\n:::\n::::\n\n::: {.content-visible when-format=\"html\" style=\"text-align: justify\"}\nLos resultados en la Figura @fig-Temperatura de Temperatura (° F) indican que la mediana de los pronósticos a 7 días, 4 días y 2 días utilizando la red neuronal LSTM presenta una aproximación más precisa a los datos históricos que la pronosticada por la red neuronal convolucional.\n\n<!-- Y también para los 4 días correspondientes a 08 de Abril del 2009 al 12 de Abril al 2009. -->\n\n<!-- 08 de febrero del 2009 al 11 de Febrero del 2009. -->\n\n<!-- 23 de Enero de 2009 al 25 de Junio de 2009. -->\n:::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-pmbar}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-25-1.pdf)\n:::\n:::\n\n\n\n\n\n\nPredicción de 7 días, 4 días y 2 días de la presión del aire p(mbar) utilizando la red neuronal convolucional y la red neuronal LSTM.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-rho}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-26-1.pdf)\n:::\n:::\n\n\n\n\n\n\nPredicción de 7 días, 4 días y 2 días de la densidad del aire utilizando la red neuronal convolucional y la red neuronal LSTM.\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-rh}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-27-1.pdf)\n:::\n:::\n\n\n\n\n\n\nPredicción de 7 días, 4 días y 2 días de la humedad relativa utilizando la red neuronal convolucional y la red neuronal LSTM\n:::\n::::\n\n:::: {.content-visible when-format=\"html\"}\n::: {#fig-wv}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](estudio_files/figure-pdf/unnamed-chunk-28-1.pdf)\n:::\n:::\n\n\n\n\n\n\nPredicción de 7 días, 4 días y 2 días de la velocidad del viento utilizando la red neuronal convolucional y la red neuronal LSTM\n:::\n::::\n\n::: {.content-visible when-format=\"pdf\"}\n![Predicción de 7 días, 4 días y 2 días de la Temperatura (° F) utilizando la red neuronal convolucional y la red neuronal LSTM](Temperatura2.png){#fig-Temperaturapdf width=\"515\"}\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\nLos resultados en la Figura de Temperatura (@fig-Temperaturapdf) indican que la [mediana](Estadistica.qmd#sec-Mediana) de los pronósticos a 7 días, 4 días y 2 días utilizando la red neuronal LSTM presenta una aproximación más precisa a los datos históricos que la pronosticada por la red neuronal convolucional.\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Predicción de 7 días, 4 días y 2 días de la presión del aire utilizando la red neuronal convolucional y la red neuronal LSTM.](PMBAR.png){#fig-pmbarpdf}\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Predicción de 7 días, 4 días y 2 días de la densidad del aire utilizando la red neuronal convolucional y la red neuronal LSTM.](RHO.png){#fig-rhopdf}\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Predicción de 7 días, 4 días y 2 días de la humedad relativa utilizando la red neuronal convolucional y la red neuronal LSTM](RH.png){#fig-rhpdf}\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n![Predicción de 7 días, 4 días y 2 días de la velocidad del viento utilizando la red neuronal convolucional y la red neuronal LSTM](WV.png){#fig-wvpdf}\n:::\n\n::: {.content-visible when-format=\"pdf\" style=\"text-align: justify\"}\n\\newpage\n:::\n\n#### RMSE\n\n::: {style=\"text-align: justify\"}\nLuego de obtener los valores pronosticados por las redes propuestas, se tomó la [mediana](Estadistica.qmd#sec-Mediana) de los pronósticos y se procedió a contrastarlos con el error cuadrático medio ([RMSE](series.qmd#sec-RMSE)). Esto permitió determinar el menor valor del error y así identificar diferencias de usa los modelos de redes. Los resultados obtenidos se muestran en @tbl-RMSE.\n:::\n\n|  |  |  |  |  |\n|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|  |  | **RMSE** |  |  |\n| **Datos históricos** | **180 días** | **90 días** | **30 días** | **15 días** |\n| **Numero de días pronosticados** | 7 días | 4 días | 4 días | 2 días |\n| **Temperatura (CNN)** | 7.748363 | 12.544 | 4.729259 | 5.89646 |\n| **Temperatura (LSTM)** | 3.035672 | 7.350131 | 3.267742 | 4.281648 |\n| **Velocidad del viento (CNN)** | 1.805565 | 1.525929 | 2.68279 | 3.638407 |\n| **Velocidad del viento (LSTM)** | 1.415952 | 1.366431 | 2.711044 | 2.846553 |\n| **Densidad del aire (CNN)** | 43.71869 | 24.26363 | 26.00116 | 69.96927 |\n| **Densidad del aire (LSTM)** | 14.33086 | 43.15156 | 22.59924 | 38.28266 |\n| **Humedad relativa (CNN)** | 23.77749 | 34.24427 | 21.69839 | 10.64634 |\n| **Humedad relativa (LSTM)** | 10.83128 | 12.39585 | 13.42141 | 14.1402 |\n| **Presión del aire (CNN)** | 9.351951 | 19.09469 | 17.30254 | 17.22759 |\n| **Presión del aire (LSTM)** | 3.638899 | 3.289955 | 9.254557 | 22.74026 |\n\n: RMSE de la mediana de los pronósticos para 5 variables. {#tbl-RMSE}\n\n::: {style=\"text-align: justify\"}\nA través de la @tbl-RMSE, se observa inicialmente de que para estos datos meteorológicos, la red neuronal LSTM exhibe un error considerablemente menor en comparación con la red neuronal convolucional. Esto resulta en un pronóstico mas cercano a los valores reales, por lo tanto, se recomienda utilizar la red neuronal LSTM para llevar a cabo futuras predicciones.\n\nEn la @tbl-RMSE se observa que la red neuronal LSTM proporciona una mejor aproximación para la variable velocidad del viento, ya que el error cuadrático medio ([RMSE](series.qmd#sec-RMSE)) es significativamente menor en todos los pronóstico realizados en comparación con la red neuronal convolucional. Además, el mismo análisis en la @tbl-RMSE revela que, a medida que aumentan los datos históricos de la velocidad del viento, el RMSE tiende a disminuir.\n\nPor otro lado, se observa en @tbl-RMSE que el pronóstico para la densidad del aire no es tan preciso, ya que presenta el ([RMSE](series.qmd#sec-RMSE)) más alto en comparación con las demás variables. En ambos modelos, tanto la red neuronal LSTM como la red neuronal convolucional, no logran una buena predicción en los días pronosticados, comparado con la gráfica del valor real. Esta diferencia en la precisión del pronóstico podría deberse a varios factores. Uno de los factores potenciales es una posible falla en los sensores de densidad del aire. Los sensores podrían estar proporcionando datos inconsistentes, lo que afectaría la capacidad de los modelos para aprender patrones precisos y hacer predicciones confiables. Además, es posible que haya fluctuaciones no detectadas en las condiciones ambientales o interferencias externas que impacten la precisión de los sensores.\n:::\n",
    "supporting": [
      "estudio_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}